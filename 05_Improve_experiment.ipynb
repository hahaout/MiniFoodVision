{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Now we have chose FoodVision from PyTorvh torchvision to work with\n",
    "\n",
    "We need to:\n",
    "1. Prepare our data from torch vision\n",
    "2. Build a model( we will use pre-build model to make comparison)\n",
    "    * 1. Choose an optimizer and loss function\n",
    "    * 2. Design a training and testing loop\n",
    "3. Fit the model to the data and make a prediction\n",
    "4. Evaluate the model\n",
    "5. Improve through experiment\n",
    "6. Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improve through experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x2884ec6c390>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x2884ecda010>,\n",
       " 101)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the data\n",
    "\n",
    "# Set image path\n",
    "from pathlib import Path\n",
    "from scripts import data_setup\n",
    "\n",
    "image_data = Path(\"data/\") / \"food_10_percent\"\n",
    "\n",
    "train_dir = image_data / \"train\"\n",
    "test_dir = image_data / \"test\"\n",
    "\n",
    "# Setup train and test transform\n",
    "from torchvision import transforms\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# create dataloader\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                               test_dir=test_dir,\n",
    "                                                                               train_transforms=train_transform,\n",
    "                                                                               test_transforms=test_transform,\n",
    "                                                                               batch_size=32,\n",
    "                                                                               )\n",
    "\n",
    "train_dataloader, test_dataloader, len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape               Output Shape              Param #                   Trainable\n",
       "================================================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]          [1, 101]                  --                        Partial\n",
       "├─Sequential (features)                                      [1, 3, 224, 224]          [1, 1280, 7, 7]           --                        False\n",
       "│    └─Conv2dNormActivation (0)                              [1, 3, 224, 224]          [1, 32, 112, 112]         --                        False\n",
       "│    │    └─Conv2d (0)                                       [1, 3, 224, 224]          [1, 32, 112, 112]         (864)                     False\n",
       "│    │    └─BatchNorm2d (1)                                  [1, 32, 112, 112]         [1, 32, 112, 112]         (64)                      False\n",
       "│    │    └─SiLU (2)                                         [1, 32, 112, 112]         [1, 32, 112, 112]         --                        --\n",
       "│    └─Sequential (1)                                        [1, 32, 112, 112]         [1, 16, 112, 112]         --                        False\n",
       "│    │    └─MBConv (0)                                       [1, 32, 112, 112]         [1, 16, 112, 112]         (1,448)                   False\n",
       "│    └─Sequential (2)                                        [1, 16, 112, 112]         [1, 24, 56, 56]           --                        False\n",
       "│    │    └─MBConv (0)                                       [1, 16, 112, 112]         [1, 24, 56, 56]           (6,004)                   False\n",
       "│    │    └─MBConv (1)                                       [1, 24, 56, 56]           [1, 24, 56, 56]           (10,710)                  False\n",
       "│    └─Sequential (3)                                        [1, 24, 56, 56]           [1, 40, 28, 28]           --                        False\n",
       "│    │    └─MBConv (0)                                       [1, 24, 56, 56]           [1, 40, 28, 28]           (15,350)                  False\n",
       "│    │    └─MBConv (1)                                       [1, 40, 28, 28]           [1, 40, 28, 28]           (31,290)                  False\n",
       "│    └─Sequential (4)                                        [1, 40, 28, 28]           [1, 80, 14, 14]           --                        False\n",
       "│    │    └─MBConv (0)                                       [1, 40, 28, 28]           [1, 80, 14, 14]           (37,130)                  False\n",
       "│    │    └─MBConv (1)                                       [1, 80, 14, 14]           [1, 80, 14, 14]           (102,900)                 False\n",
       "│    │    └─MBConv (2)                                       [1, 80, 14, 14]           [1, 80, 14, 14]           (102,900)                 False\n",
       "│    └─Sequential (5)                                        [1, 80, 14, 14]           [1, 112, 14, 14]          --                        False\n",
       "│    │    └─MBConv (0)                                       [1, 80, 14, 14]           [1, 112, 14, 14]          (126,004)                 False\n",
       "│    │    └─MBConv (1)                                       [1, 112, 14, 14]          [1, 112, 14, 14]          (208,572)                 False\n",
       "│    │    └─MBConv (2)                                       [1, 112, 14, 14]          [1, 112, 14, 14]          (208,572)                 False\n",
       "│    └─Sequential (6)                                        [1, 112, 14, 14]          [1, 192, 7, 7]            --                        False\n",
       "│    │    └─MBConv (0)                                       [1, 112, 14, 14]          [1, 192, 7, 7]            (262,492)                 False\n",
       "│    │    └─MBConv (1)                                       [1, 192, 7, 7]            [1, 192, 7, 7]            (587,952)                 False\n",
       "│    │    └─MBConv (2)                                       [1, 192, 7, 7]            [1, 192, 7, 7]            (587,952)                 False\n",
       "│    │    └─MBConv (3)                                       [1, 192, 7, 7]            [1, 192, 7, 7]            (587,952)                 False\n",
       "│    └─Sequential (7)                                        [1, 192, 7, 7]            [1, 320, 7, 7]            --                        False\n",
       "│    │    └─MBConv (0)                                       [1, 192, 7, 7]            [1, 320, 7, 7]            (717,232)                 False\n",
       "│    └─Conv2dNormActivation (8)                              [1, 320, 7, 7]            [1, 1280, 7, 7]           --                        False\n",
       "│    │    └─Conv2d (0)                                       [1, 320, 7, 7]            [1, 1280, 7, 7]           (409,600)                 False\n",
       "│    │    └─BatchNorm2d (1)                                  [1, 1280, 7, 7]           [1, 1280, 7, 7]           (2,560)                   False\n",
       "│    │    └─SiLU (2)                                         [1, 1280, 7, 7]           [1, 1280, 7, 7]           --                        --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [1, 1280, 7, 7]           [1, 1280, 1, 1]           --                        --\n",
       "├─Sequential (classifier)                                    [1, 1280]                 [1, 101]                  --                        True\n",
       "│    └─Dropout (0)                                           [1, 1280]                 [1, 1280]                 --                        --\n",
       "│    └─Linear (1)                                            [1, 1280]                 [1, 101]                  129,381                   True\n",
       "================================================================================================================================================================\n",
       "Total params: 4,136,929\n",
       "Trainable params: 129,381\n",
       "Non-trainable params: 4,007,548\n",
       "Total mult-adds (Units.MEGABYTES): 384.72\n",
       "================================================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 107.88\n",
       "Params size (MB): 16.55\n",
       "Estimated Total Size (MB): 125.03\n",
       "================================================================================================================================================================"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as T\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "# device agnostic code\n",
    "device = \"cuda\" if T.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# create effnet b0\n",
    "b0_weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "Effnet_b0 = torchvision.models.efficientnet_b0(weights=b0_weights)\n",
    "\n",
    "# Freeze the feature layer\n",
    "for params in Effnet_b0.features.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# Change the classifier\n",
    "\n",
    "## Before changeing we need to know effnet_b0 info by using summary\n",
    "\"\"\"summary(model=Effnet_b0,\n",
    "        input_size=(1,3,224,224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        row_settings=[\"var_names\"])\"\"\"\n",
    "\n",
    "## From the above we can know Linear layer takes in 1280 and out put 1000\n",
    "# Change classifer to own preference\n",
    "\n",
    "Effnet_b0.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=len(class_names))\n",
    ")\n",
    "\n",
    "## Print out to make sure\n",
    "summary(model=Effnet_b0,\n",
    "        input_size=(1,3,224,224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        row_settings=[\"var_names\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Let's improve our train() from `engine.py` with Summary Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.engine import train_step, test_step\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def train(model:nn.Module,\n",
    "          train_dataloader:T.utils.data.DataLoader,\n",
    "          test_dataloader:T.utils.data.DataLoader,\n",
    "          loss_fn:nn.Module,\n",
    "          optimizer:T.optim.Optimizer,\n",
    "          epochs:int,\n",
    "          device:T.device,\n",
    "          writer:T.utils.tensorboard.SummaryWriter):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Create empty results dictionary\n",
    "        results = {\"train_loss\": [],\n",
    "                \"train_acc\": [],\n",
    "                \"test_loss\": [],\n",
    "                \"test_acc\": []\n",
    "        }\n",
    "        model.to(device)\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                        train_dataloader=train_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        optimizer=optimizer,\n",
    "                                        device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        test_dataloader=test_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "        ### EXPERIMENT ###\n",
    "        writer.add_scalars(main_tag=\"Accuracy\",\n",
    "                          tag_scalar_dict={\"train_acc\":train_acc,\n",
    "                                           \"test_acc\" :test_acc},\n",
    "                            global_step = epoch)\n",
    "        writer.add_scalars(main_tag=\"Loss\",\n",
    "                          tag_scalar_dict={\"train_loss\": train_loss,\n",
    "                                           \"test_loss\": test_loss},\n",
    "                                           global_step=epoch)\n",
    "        writer.add_graph(model=model,\n",
    "                         input_to_model=T.rand(32,3,224,224).to(device))\n",
    "        writer.close()\n",
    "\n",
    "        ### END ###\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is writtern lets replace it into our engine.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "def create_writer(experiment_name: str,\n",
    "                  model_name: str,\n",
    "                  extra: str = None):\n",
    "    \"\"\" \n",
    "    Creates a torch.utils.tensorboard.writer.SummaryWriter() instance tracking to a specific\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    # Get timestamp of current date in reverse order\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    if extra:\n",
    "        # Create log directory\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
    "    else:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "    print(f\"[INFO] Created SummaryWriter saving to {log_dir}\")\n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created SummaryWriter saving to runs\\2024-10-20\\10%_data\\Effnet_b0\\5_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214e380b14db4930a52b7240786e27f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 25.2052 | train_acc: 0.1502 | test_loss: 22.5275 | test_acc: 0.3171\n",
      "Epoch: 2 | train_loss: 25.7342 | train_acc: 0.2738 | test_loss: 26.2076 | test_acc: 0.3186\n",
      "Epoch: 3 | train_loss: 25.7159 | train_acc: 0.3297 | test_loss: 27.5323 | test_acc: 0.3501\n",
      "Epoch: 4 | train_loss: 26.1466 | train_acc: 0.3588 | test_loss: 28.5846 | test_acc: 0.3631\n",
      "Epoch: 5 | train_loss: 26.6033 | train_acc: 0.3766 | test_loss: 29.9920 | test_acc: 0.3724\n"
     ]
    }
   ],
   "source": [
    "# Train our effnet_b0 with our data\n",
    "## Setup loss and optimizer\n",
    "from scripts import engine\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = T.optim.Adam(params=Effnet_b0.parameters(),\n",
    "                         lr=0.1)\n",
    "\n",
    "# Get result\n",
    "Effnetb0_results = engine.train(model=Effnet_b0,\n",
    "                         train_dataloader=train_dataloader,\n",
    "                         test_dataloader=test_dataloader,\n",
    "                         loss_fn=loss_fn,\n",
    "                         optimizer=optimizer,\n",
    "                         epochs=5,\n",
    "                         device=device,\n",
    "                         writer=create_writer(experiment_name=\"10%_data\",\n",
    "                                              model_name=\"Effnet_b0\",\n",
    "                                              extra=\"5_epochs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 it seems our model is training but could we use other effnet to train better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to build effnet_b0, efffnet_b2, effnet_b4\n",
    "\n",
    "def create_b0():\n",
    "    # create effnet b0\n",
    "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "\n",
    "    # Freeze the feature layer\n",
    "    for params in model.features.parameters():\n",
    "        params.requires_grad = False\n",
    "\n",
    "\n",
    "    # Change classifer to own preference\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(in_features=1280, out_features=len(class_names))\n",
    "    ).to(device)\n",
    "\n",
    "    # Give the model a name\n",
    "    model.name = \"effnetb0\"\n",
    "    print(f\"[INFO] Created new {model.name} model...\")\n",
    "    return model\n",
    "\n",
    "def create_b2():\n",
    "    # create effnet b0\n",
    "    b2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b2(weights=b2_weights)\n",
    "\n",
    "    # Freeze the feature layer\n",
    "    for params in model.features.parameters():\n",
    "        params.requires_grad = False\n",
    "\n",
    "\n",
    "    # Change classifer to own preference\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=len(class_names))\n",
    "    )\n",
    "    # Give the model a name\n",
    "    model.name = \"effnetb2\"\n",
    "    print(f\"[INFO] Created new {model.name} model...\")\n",
    "    return model\n",
    "\n",
    "def create_b4():\n",
    "    # create effnet b0\n",
    "    b4_weights = torchvision.models.EfficientNet_B4_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b4(weights=b4_weights)\n",
    "\n",
    "    # Freeze the feature layer\n",
    "    for params in model.features.parameters():\n",
    "        params.requires_grad = False\n",
    "\n",
    "\n",
    "    # Change classifer to own preference\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.4, inplace=True),\n",
    "        nn.Linear(in_features=1792, out_features=len(class_names))\n",
    "    )\n",
    "    # Give the model a name\n",
    "    model.name = \"effnetb4\"\n",
    "    print(f\"[INFO] Created new {model.name} model...\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created new effnetb4 model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape               Output Shape              Param #                   Trainable\n",
       "================================================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]          [1, 101]                  --                        Partial\n",
       "├─Sequential (features)                                      [1, 3, 224, 224]          [1, 1792, 7, 7]           --                        False\n",
       "│    └─Conv2dNormActivation (0)                              [1, 3, 224, 224]          [1, 48, 112, 112]         --                        False\n",
       "│    │    └─Conv2d (0)                                       [1, 3, 224, 224]          [1, 48, 112, 112]         (1,296)                   False\n",
       "│    │    └─BatchNorm2d (1)                                  [1, 48, 112, 112]         [1, 48, 112, 112]         (96)                      False\n",
       "│    │    └─SiLU (2)                                         [1, 48, 112, 112]         [1, 48, 112, 112]         --                        --\n",
       "│    └─Sequential (1)                                        [1, 48, 112, 112]         [1, 24, 112, 112]         --                        False\n",
       "│    │    └─MBConv (0)                                       [1, 48, 112, 112]         [1, 24, 112, 112]         (2,940)                   False\n",
       "│    │    └─MBConv (1)                                       [1, 24, 112, 112]         [1, 24, 112, 112]         (1,206)                   False\n",
       "│    └─Sequential (2)                                        [1, 24, 112, 112]         [1, 32, 56, 56]           --                        False\n",
       "│    │    └─MBConv (0)                                       [1, 24, 112, 112]         [1, 32, 56, 56]           (11,878)                  False\n",
       "│    │    └─MBConv (1)                                       [1, 32, 56, 56]           [1, 32, 56, 56]           (18,120)                  False\n",
       "│    │    └─MBConv (2)                                       [1, 32, 56, 56]           [1, 32, 56, 56]           (18,120)                  False\n",
       "│    │    └─MBConv (3)                                       [1, 32, 56, 56]           [1, 32, 56, 56]           (18,120)                  False\n",
       "│    └─Sequential (3)                                        [1, 32, 56, 56]           [1, 56, 28, 28]           --                        False\n",
       "│    │    └─MBConv (0)                                       [1, 32, 56, 56]           [1, 56, 28, 28]           (25,848)                  False\n",
       "│    │    └─MBConv (1)                                       [1, 56, 28, 28]           [1, 56, 28, 28]           (57,246)                  False\n",
       "│    │    └─MBConv (2)                                       [1, 56, 28, 28]           [1, 56, 28, 28]           (57,246)                  False\n",
       "│    │    └─MBConv (3)                                       [1, 56, 28, 28]           [1, 56, 28, 28]           (57,246)                  False\n",
       "│    └─Sequential (4)                                        [1, 56, 28, 28]           [1, 112, 14, 14]          --                        False\n",
       "│    │    └─MBConv (0)                                       [1, 56, 28, 28]           [1, 112, 14, 14]          (70,798)                  False\n",
       "│    │    └─MBConv (1)                                       [1, 112, 14, 14]          [1, 112, 14, 14]          (197,820)                 False\n",
       "│    │    └─MBConv (2)                                       [1, 112, 14, 14]          [1, 112, 14, 14]          (197,820)                 False\n",
       "│    │    └─MBConv (3)                                       [1, 112, 14, 14]          [1, 112, 14, 14]          (197,820)                 False\n",
       "│    │    └─MBConv (4)                                       [1, 112, 14, 14]          [1, 112, 14, 14]          (197,820)                 False\n",
       "│    │    └─MBConv (5)                                       [1, 112, 14, 14]          [1, 112, 14, 14]          (197,820)                 False\n",
       "│    └─Sequential (5)                                        [1, 112, 14, 14]          [1, 160, 14, 14]          --                        False\n",
       "│    │    └─MBConv (0)                                       [1, 112, 14, 14]          [1, 160, 14, 14]          (240,924)                 False\n",
       "│    │    └─MBConv (1)                                       [1, 160, 14, 14]          [1, 160, 14, 14]          (413,160)                 False\n",
       "│    │    └─MBConv (2)                                       [1, 160, 14, 14]          [1, 160, 14, 14]          (413,160)                 False\n",
       "│    │    └─MBConv (3)                                       [1, 160, 14, 14]          [1, 160, 14, 14]          (413,160)                 False\n",
       "│    │    └─MBConv (4)                                       [1, 160, 14, 14]          [1, 160, 14, 14]          (413,160)                 False\n",
       "│    │    └─MBConv (5)                                       [1, 160, 14, 14]          [1, 160, 14, 14]          (413,160)                 False\n",
       "│    └─Sequential (6)                                        [1, 160, 14, 14]          [1, 272, 7, 7]            --                        False\n",
       "│    │    └─MBConv (0)                                       [1, 160, 14, 14]          [1, 272, 7, 7]            (520,904)                 False\n",
       "│    │    └─MBConv (1)                                       [1, 272, 7, 7]            [1, 272, 7, 7]            (1,159,332)               False\n",
       "│    │    └─MBConv (2)                                       [1, 272, 7, 7]            [1, 272, 7, 7]            (1,159,332)               False\n",
       "│    │    └─MBConv (3)                                       [1, 272, 7, 7]            [1, 272, 7, 7]            (1,159,332)               False\n",
       "│    │    └─MBConv (4)                                       [1, 272, 7, 7]            [1, 272, 7, 7]            (1,159,332)               False\n",
       "│    │    └─MBConv (5)                                       [1, 272, 7, 7]            [1, 272, 7, 7]            (1,159,332)               False\n",
       "│    │    └─MBConv (6)                                       [1, 272, 7, 7]            [1, 272, 7, 7]            (1,159,332)               False\n",
       "│    │    └─MBConv (7)                                       [1, 272, 7, 7]            [1, 272, 7, 7]            (1,159,332)               False\n",
       "│    └─Sequential (7)                                        [1, 272, 7, 7]            [1, 448, 7, 7]            --                        False\n",
       "│    │    └─MBConv (0)                                       [1, 272, 7, 7]            [1, 448, 7, 7]            (1,420,804)               False\n",
       "│    │    └─MBConv (1)                                       [1, 448, 7, 7]            [1, 448, 7, 7]            (3,049,200)               False\n",
       "│    └─Conv2dNormActivation (8)                              [1, 448, 7, 7]            [1, 1792, 7, 7]           --                        False\n",
       "│    │    └─Conv2d (0)                                       [1, 448, 7, 7]            [1, 1792, 7, 7]           (802,816)                 False\n",
       "│    │    └─BatchNorm2d (1)                                  [1, 1792, 7, 7]           [1, 1792, 7, 7]           (3,584)                   False\n",
       "│    │    └─SiLU (2)                                         [1, 1792, 7, 7]           [1, 1792, 7, 7]           --                        --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [1, 1792, 7, 7]           [1, 1792, 1, 1]           --                        --\n",
       "├─Sequential (classifier)                                    [1, 1792]                 [1, 101]                  --                        True\n",
       "│    └─Dropout (0)                                           [1, 1792]                 [1, 1792]                 --                        --\n",
       "│    └─Linear (1)                                            [1, 1792]                 [1, 101]                  181,093                   True\n",
       "================================================================================================================================================================\n",
       "Total params: 17,729,709\n",
       "Trainable params: 181,093\n",
       "Non-trainable params: 17,548,616\n",
       "Total mult-adds (Units.GIGABYTES): 1.50\n",
       "================================================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 272.49\n",
       "Params size (MB): 70.92\n",
       "Estimated Total Size (MB): 344.01\n",
       "================================================================================================================================================================"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = create_b4()\n",
    "summary(model=dummy,\n",
    "        input_size=(1,3,224,224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Experiment number: 1\n",
      "[INFO] Model: effnetb0\n",
      "[INFO] DataLoader: train_dataloader\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created new effnetb0 model...\n",
      "[INFO] Created SummaryWriter saving to runs\\2024-10-20\\train_dataloader\\effnetb0\\5_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e71b50efb8740838e1e16d7b4ae8a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 3.9300 | train_acc: 0.1764 | test_loss: 3.0561 | test_acc: 0.3726\n",
      "Epoch: 2 | train_loss: 2.9651 | train_acc: 0.3601 | test_loss: 2.5536 | test_acc: 0.4300\n",
      "Epoch: 3 | train_loss: 2.5814 | train_acc: 0.4227 | test_loss: 2.3522 | test_acc: 0.4550\n",
      "Epoch: 4 | train_loss: 2.3640 | train_acc: 0.4543 | test_loss: 2.2351 | test_acc: 0.4744\n",
      "Epoch: 5 | train_loss: 2.2183 | train_acc: 0.4736 | test_loss: 2.1843 | test_acc: 0.4729\n",
      "[INFO] Saving model to: models\\07_effnetb0_train_dataloader_5_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 2\n",
      "[INFO] Model: effnetb2\n",
      "[INFO] DataLoader: train_dataloader\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created new effnetb2 model...\n",
      "[INFO] Created SummaryWriter saving to runs\\2024-10-20\\train_dataloader\\effnetb2\\5_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15b5213b63f4a3a81f916da6cdac776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 4.0215 | train_acc: 0.1655 | test_loss: 3.3010 | test_acc: 0.3412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000028847E72E80>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Study\\ML_learning\\myenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"d:\\Study\\ML_learning\\myenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1562, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 3.0812 | train_acc: 0.3520 | test_loss: 2.7496 | test_acc: 0.4071\n",
      "Epoch: 3 | train_loss: 2.6735 | train_acc: 0.4071 | test_loss: 2.5293 | test_acc: 0.4288\n",
      "Epoch: 4 | train_loss: 2.4354 | train_acc: 0.4470 | test_loss: 2.3890 | test_acc: 0.4451\n",
      "Epoch: 5 | train_loss: 2.3010 | train_acc: 0.4650 | test_loss: 2.2860 | test_acc: 0.4569\n",
      "[INFO] Saving model to: models\\07_effnetb2_train_dataloader_5_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 3\n",
      "[INFO] Model: effnetb4\n",
      "[INFO] DataLoader: train_dataloader\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created new effnetb4 model...\n",
      "[INFO] Created SummaryWriter saving to runs\\2024-10-20\\train_dataloader\\effnetb4\\5_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853a8a9d039540dd896103b8a2bf7815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 4.3121 | train_acc: 0.1390 | test_loss: 3.9117 | test_acc: 0.3114\n",
      "Epoch: 2 | train_loss: 3.7023 | train_acc: 0.2975 | test_loss: 3.4195 | test_acc: 0.3628\n",
      "Epoch: 3 | train_loss: 3.2916 | train_acc: 0.3588 | test_loss: 3.1136 | test_acc: 0.3758\n",
      "Epoch: 4 | train_loss: 3.0175 | train_acc: 0.3839 | test_loss: 2.8812 | test_acc: 0.4060\n",
      "Epoch: 5 | train_loss: 2.8101 | train_acc: 0.4072 | test_loss: 2.7065 | test_acc: 0.4167\n",
      "[INFO] Saving model to: models\\07_effnetb4_train_dataloader_5_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 4\n",
      "[INFO] Model: effnetb0\n",
      "[INFO] DataLoader: train_dataloader\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created new effnetb0 model...\n",
      "[INFO] Created SummaryWriter saving to runs\\2024-10-20\\train_dataloader\\effnetb0\\10_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe84687392c34cf8b011d78b546fc29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 3.9332 | train_acc: 0.1805 | test_loss: 3.0786 | test_acc: 0.3674\n",
      "Epoch: 2 | train_loss: 2.9614 | train_acc: 0.3647 | test_loss: 2.5675 | test_acc: 0.4334\n",
      "Epoch: 3 | train_loss: 2.5864 | train_acc: 0.4209 | test_loss: 2.3436 | test_acc: 0.4508\n",
      "Epoch: 4 | train_loss: 2.3771 | train_acc: 0.4563 | test_loss: 2.2539 | test_acc: 0.4701\n",
      "Epoch: 5 | train_loss: 2.2165 | train_acc: 0.4845 | test_loss: 2.1796 | test_acc: 0.4748\n",
      "Epoch: 6 | train_loss: 2.1147 | train_acc: 0.5026 | test_loss: 2.1286 | test_acc: 0.4827\n",
      "Epoch: 7 | train_loss: 2.0137 | train_acc: 0.5163 | test_loss: 2.1111 | test_acc: 0.4880\n",
      "Epoch: 8 | train_loss: 1.9492 | train_acc: 0.5245 | test_loss: 2.1005 | test_acc: 0.4819\n",
      "Epoch: 9 | train_loss: 1.8942 | train_acc: 0.5447 | test_loss: 2.0860 | test_acc: 0.4845\n",
      "Epoch: 10 | train_loss: 1.8274 | train_acc: 0.5554 | test_loss: 2.0737 | test_acc: 0.4906\n",
      "[INFO] Saving model to: models\\07_effnetb0_train_dataloader_10_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 5\n",
      "[INFO] Model: effnetb2\n",
      "[INFO] DataLoader: train_dataloader\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created new effnetb2 model...\n",
      "[INFO] Created SummaryWriter saving to runs\\2024-10-20\\train_dataloader\\effnetb2\\10_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61aeb2ed1e264c35a2eeaccbb4f4c638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 4.0228 | train_acc: 0.1615 | test_loss: 3.2933 | test_acc: 0.3405\n",
      "Epoch: 2 | train_loss: 3.0580 | train_acc: 0.3541 | test_loss: 2.7603 | test_acc: 0.4075\n",
      "Epoch: 3 | train_loss: 2.6597 | train_acc: 0.4102 | test_loss: 2.5125 | test_acc: 0.4353\n",
      "Epoch: 4 | train_loss: 2.4374 | train_acc: 0.4506 | test_loss: 2.3893 | test_acc: 0.4433\n",
      "Epoch: 5 | train_loss: 2.2938 | train_acc: 0.4702 | test_loss: 2.2944 | test_acc: 0.4543\n",
      "Epoch: 6 | train_loss: 2.1730 | train_acc: 0.4903 | test_loss: 2.2408 | test_acc: 0.4622\n",
      "Epoch: 7 | train_loss: 2.0998 | train_acc: 0.4991 | test_loss: 2.2041 | test_acc: 0.4669\n",
      "Epoch: 8 | train_loss: 2.0502 | train_acc: 0.5033 | test_loss: 2.1883 | test_acc: 0.4662\n",
      "Epoch: 9 | train_loss: 1.9690 | train_acc: 0.5241 | test_loss: 2.1498 | test_acc: 0.4764\n",
      "Epoch: 10 | train_loss: 1.9394 | train_acc: 0.5283 | test_loss: 2.1268 | test_acc: 0.4781\n",
      "[INFO] Saving model to: models\\07_effnetb2_train_dataloader_10_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 6\n",
      "[INFO] Model: effnetb4\n",
      "[INFO] DataLoader: train_dataloader\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created new effnetb4 model...\n",
      "[INFO] Created SummaryWriter saving to runs\\2024-10-20\\train_dataloader\\effnetb4\\10_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc85cf30f95545b3ad6549c5775240f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 4.3118 | train_acc: 0.1420 | test_loss: 3.9183 | test_acc: 0.3126\n",
      "Epoch: 2 | train_loss: 3.7014 | train_acc: 0.3075 | test_loss: 3.4325 | test_acc: 0.3588\n",
      "Epoch: 3 | train_loss: 3.2946 | train_acc: 0.3501 | test_loss: 3.1097 | test_acc: 0.3715\n",
      "Epoch: 4 | train_loss: 3.0182 | train_acc: 0.3847 | test_loss: 2.8863 | test_acc: 0.3964\n",
      "Epoch: 5 | train_loss: 2.8034 | train_acc: 0.4178 | test_loss: 2.7225 | test_acc: 0.4190\n",
      "Epoch: 6 | train_loss: 2.6646 | train_acc: 0.4260 | test_loss: 2.6176 | test_acc: 0.4291\n",
      "Epoch: 7 | train_loss: 2.5489 | train_acc: 0.4443 | test_loss: 2.5255 | test_acc: 0.4349\n",
      "Epoch: 8 | train_loss: 2.4497 | train_acc: 0.4593 | test_loss: 2.4487 | test_acc: 0.4460\n",
      "Epoch: 9 | train_loss: 2.3905 | train_acc: 0.4637 | test_loss: 2.3688 | test_acc: 0.4571\n",
      "Epoch: 10 | train_loss: 2.2917 | train_acc: 0.4708 | test_loss: 2.3411 | test_acc: 0.4492\n",
      "[INFO] Saving model to: models\\07_effnetb4_train_dataloader_10_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "from scripts.save_model import save_model\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = [5, 10]\n",
    "models = [\"effnetb0\",\"effnetb2\",\"effnetb4\"]\n",
    "train_dataloader_set ={\"train_dataloader\":train_dataloader}\n",
    "\n",
    "# Keep track of experiment numbers\n",
    "experiment_number = 0\n",
    "\n",
    "# Loop through each dataloader\n",
    "for dataloader_name, train_dataloader in train_dataloader_set.items():\n",
    "    # Loop through the epochs\n",
    "    for epochs in num_epochs:\n",
    "        # Loop through each model name and create a new model instance\n",
    "        for model_name in models:\n",
    "            #Print out info\n",
    "            experiment_number +=1\n",
    "            print(f\"[INFO] Experiment number: {experiment_number}\")\n",
    "            print(f\"[INFO] Model: {model_name}\")\n",
    "            print(f\"[INFO] DataLoader: {dataloader_name}\")\n",
    "            print(f\"[INFO] Number of epochs: {epochs}\")\n",
    "\n",
    "            # Select and create the model\n",
    "            if model_name == \"effnetb0\":\n",
    "                model=create_b0()\n",
    "            elif model_name == \"effnetb2\" :\n",
    "                model=create_b2()\n",
    "            else:\n",
    "                model=create_b4()\n",
    "            \n",
    "            # Create a new loss and optimizer for every model\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            optimizer = T.optim.Adam(params=model.parameters(),\n",
    "                                     lr=0.001)\n",
    "            \n",
    "            # Train target model with target dataloader and track experiment\n",
    "            # Note: using train() rather engine,.train() coz this train has been modified\n",
    "            engine.train(model=model,\n",
    "                  train_dataloader=train_dataloader,\n",
    "                  test_dataloader=test_dataloader,\n",
    "                  loss_fn=loss_fn,\n",
    "                  optimizer=optimizer,\n",
    "                  epochs=epochs,\n",
    "                  device=device,\n",
    "                  writer=create_writer(experiment_name=dataloader_name,\n",
    "                                       model_name=model_name,\n",
    "                                       extra=f\"{epochs}_epochs\"))\n",
    "            \n",
    "            #Save the model to file so we can import it later if need be\n",
    "            save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n",
    "\n",
    "            save_model(model=model,\n",
    "                       target_dir=\"models\",\n",
    "                       model_name=save_filepath)\n",
    "            print(\"-\"*50 +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d41c2061e80bb3ce\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d41c2061e80bb3ce\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/train_dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
